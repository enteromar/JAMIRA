Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	resistome_prediction
	1

[Wed Aug 28 16:47:16 2019]
rule resistome_prediction:
    input: data/samples/V583.fasta
    output: resistome_prediction/V583.txt, resistome_prediction/V583.json
    jobid: 0
    wildcards: sample=V583

docker run -v $PWD:/data -t quay.io/biocontainers/rgi:4.2.2--py35ha92aebf_1 rgi main -i data/data/samples/V583.fasta -o data/resistome_prediction/V583 -t contig --clean
[Wed Aug 28 16:49:07 2019]
Error in rule resistome_prediction:
    jobid: 0
    output: resistome_prediction/V583.txt, resistome_prediction/V583.json
    shell:
        docker run -v $PWD:/data -t quay.io/biocontainers/rgi:4.2.2--py35ha92aebf_1 rgi main -i data/data/samples/V583.fasta -o data/resistome_prediction/V583 -t contig --clean
        (exited with non-zero exit code)

Removing output files of failed job resistome_prediction since they might be corrupted:
resistome_prediction/V583.txt, resistome_prediction/V583.json
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/icaro/WGCA/.snakemake/log/2019-08-28T164716.150398.snakemake.log
