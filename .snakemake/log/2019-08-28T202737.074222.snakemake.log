Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	resistome_prediction
	1

[Wed Aug 28 20:27:37 2019]
rule resistome_prediction:
    input: data/samples/C7.fasta
    output: resistome_prediction/C7, resistome_prediction/C7.txt, resistome_prediction/C7.json
    jobid: 0
    wildcards: sample=C7

docker run -v $PWD:/data -t quay.io/biocontainers/rgi:4.2.2--py35ha92aebf_1 rgi main -i data/data/samples/C7.fasta -o data/resistome_prediction/C7 -t contig --clean --debug > resistome_prediction/C7
[Wed Aug 28 20:29:24 2019]
Error in rule resistome_prediction:
    jobid: 0
    output: resistome_prediction/C7, resistome_prediction/C7.txt, resistome_prediction/C7.json
    shell:
        docker run -v $PWD:/data -t quay.io/biocontainers/rgi:4.2.2--py35ha92aebf_1 rgi main -i data/data/samples/C7.fasta -o data/resistome_prediction/C7 -t contig --clean --debug > resistome_prediction/C7
        (exited with non-zero exit code)

Removing output files of failed job resistome_prediction since they might be corrupted:
resistome_prediction/C7, resistome_prediction/C7.txt, resistome_prediction/C7.json
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/icaro/WGCA/.snakemake/log/2019-08-28T202737.074222.snakemake.log
