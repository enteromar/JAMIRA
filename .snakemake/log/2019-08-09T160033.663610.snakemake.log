Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	3	resistance_identification
	1	summarize_resistance
	1	summarize_virulence
	1	summary_results
	3	virulence_identification
	9

[Fri Aug  9 16:00:33 2019]
rule resistance_identification:
    input: data/samples/V583.fasta
    output: resistance_genes/V583.tsv
    jobid: 5
    wildcards: sample=V583

[Fri Aug  9 16:00:33 2019]
Error in rule resistance_identification:
    jobid: 5
    output: resistance_genes/V583.tsv

RuleException:
CalledProcessError in line 39 of /home/bioinfo/WGCA/Snakefile:
Command ' set -euo pipefail;  abricate --minid=90 --db=card data/samples/V583.fasta > resistance_genes/V583.tsv ' returned non-zero exit status 127.
  File "/home/bioinfo/WGCA/Snakefile", line 39, in __rule_resistance_identification
  File "/home/bioinfo/miniconda3/envs/snakemake_tutorial/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job resistance_identification since they might be corrupted:
resistance_genes/V583.tsv
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/bioinfo/WGCA/.snakemake/log/2019-08-09T160033.663610.snakemake.log
